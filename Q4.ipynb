{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6+AQ3vyG/yRraOZRO6XA4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjmousavi97/Deep-Learning-Tehran-uni/blob/main/Q4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Gradient Descent in MLP â€“ Derivation and Update Rules\n",
        "\n",
        "---\n",
        "\n",
        "## Notation\n",
        "\n",
        "- $N$: Total number of training samples (full batch)\n",
        "- $x^{(n)}$: Input vector for sample $n$\n",
        "- $y^{(n)}$: True output (target) for sample $n$\n",
        "- $h^{(n)}$: Network output for sample $n$\n",
        "- $f(\\cdot)$: Activation function at hidden layer\n",
        "- $g(\\cdot)$: Activation function at output layer\n",
        "- Loss function:\n",
        "\n",
        "  $$\n",
        "  J = \\frac{1}{N} \\sum_{n=1}^N \\frac{1}{2} \\| h^{(n)} - y^{(n)} \\|^2\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## Forward Propagation (Per Sample)\n",
        "\n",
        "1. **Hidden layer**:\n",
        "   $$\n",
        "   z_j^{1(n)} = \\sum_i w^1_{ji} x_i^{(n)} + b^1_j\n",
        "   $$\n",
        "   $$\n",
        "   a_j^{1(n)} = f(z_j^{1(n)})\n",
        "   $$\n",
        "\n",
        "2. **Output layer**:\n",
        "   $$\n",
        "   z_k^{2(n)} = \\sum_j w^2_{kj} a_j^{1(n)} + b^2_k\n",
        "   $$\n",
        "   $$\n",
        "   h_k^{(n)} = g(z_k^{2(n)})\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "## Gradients (Full-Batch)\n",
        "\n",
        "### Output Weights $w^2_{kj}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w^2_{kj}} =\n",
        "\\frac{1}{N} \\sum_{n=1}^N (h_k^{(n)} - y_k^{(n)}) \\cdot g'(z_k^{2(n)}) \\cdot a_j^{1(n)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Output Biases $b^2_k$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b^2_k} =\n",
        "\\frac{1}{N} \\sum_{n=1}^N (h_k^{(n)} - y_k^{(n)}) \\cdot g'(z_k^{2(n)})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Hidden Weights $w^1_{ji}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w^1_{ji}} =\n",
        "\\frac{1}{N} \\sum_{n=1}^N \\sum_k\n",
        "\\left[ (h_k^{(n)} - y_k^{(n)}) \\cdot g'(z_k^{2(n)}) \\cdot w^2_{kj} \\right]\n",
        "\\cdot f'(z_j^{1(n)}) \\cdot x_i^{(n)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Hidden Biases $b^1_j$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b^1_j} =\n",
        "\\frac{1}{N} \\sum_{n=1}^N \\sum_k\n",
        "\\left[ (h_k^{(n)} - y_k^{(n)}) \\cdot g'(z_k^{2(n)}) \\cdot w^2_{kj} \\right]\n",
        "\\cdot f'(z_j^{1(n)})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Update Rules\n",
        "\n",
        "Using learning rate $\\eta$:\n",
        "\n",
        "- **Output weights**:\n",
        "  $$\n",
        "  w^2_{kj} \\leftarrow w^2_{kj} - \\eta \\cdot \\frac{\\partial J}{\\partial w^2_{kj}}\n",
        "  $$\n",
        "\n",
        "- **Output biases**:\n",
        "  $$\n",
        "  b^2_k \\leftarrow b^2_k - \\eta \\cdot \\frac{\\partial J}{\\partial b^2_k}\n",
        "  $$\n",
        "\n",
        "- **Hidden weights**:\n",
        "  $$\n",
        "  w^1_{ji} \\leftarrow w^1_{ji} - \\eta \\cdot \\frac{\\partial J}{\\partial w^1_{ji}}\n",
        "  $$\n",
        "\n",
        "- **Hidden biases**:\n",
        "  $$\n",
        "  b^1_j \\leftarrow b^1_j - \\eta \\cdot \\frac{\\partial J}{\\partial b^1_j}\n",
        "  $$\n"
      ],
      "metadata": {
        "id": "eZRe6gwZe-PT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation Derivations for Stochastic and Mini-Batch Gradient Descent\n",
        "\n",
        "This document provides detailed mathematical derivations for training a Multi-Layer Perceptron (MLP) with one hidden layer using:\n",
        "\n",
        "- **Stochastic Gradient Descent (SGD)**\n",
        "- **Mini-Batch Gradient Descent**\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Network Architecture\n",
        "\n",
        "Let the MLP be defined as follows:\n",
        "\n",
        "- Input: $ \\mathbf{x} \\in \\mathbb{R}^n $\n",
        "- Hidden layer: weights $ W^1 $, biases $ b^1 $, activation $ f $\n",
        "- Output layer: weights $ W^2 $, biases $ b^2 $, activation $ g $\n",
        "\n",
        "### Forward Propagation:\n",
        "\n",
        "- Hidden pre-activation:  \n",
        "  $$\n",
        "  z_j^2 = \\sum_{i} w_{ji}^1 x_i + b_j^1\n",
        "  $$\n",
        "\n",
        "- Hidden activation:  \n",
        "  $$\n",
        "  a_j^2 = f(z_j^2)\n",
        "  $$\n",
        "\n",
        "- Output pre-activation:  \n",
        "  $$\n",
        "  z_k^3 = \\sum_{j} w_{kj}^2 a_j^2 + b_k^2\n",
        "  $$\n",
        "\n",
        "- Output activation:  \n",
        "  $$\n",
        "  h_k = g(z_k^3)\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… 1. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "### ðŸ”¹ Loss Function:\n",
        "\n",
        "For a single training example $ (\\mathbf{x}^q, y^q) $:\n",
        "\n",
        "$$\n",
        "J^{(q)}(\\mathbf{w}, \\mathbf{b}) = \\frac{1}{2} (h_k^q - y_k^q)^2\n",
        "$$\n",
        "\n",
        "### ðŸ”¹ Gradients:\n",
        "\n",
        "#### Output Layer\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial z_k^3} = (h_k - y_k) \\cdot g'(z_k^3) $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial w_{kj}^2} = (h_k - y_k) g'(z_k^3) a_j^2 $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial b_k^2} = (h_k - y_k) g'(z_k^3) $\n",
        "\n",
        "#### Hidden Layer\n",
        "\n",
        "- Backpropagate error:  \n",
        "  $$\n",
        "  \\delta_j^2 = f'(z_j^2) \\sum_k (h_k - y_k) g'(z_k^3) w_{kj}^2\n",
        "  $$\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial w_{ji}^1} = \\delta_j^2 x_i $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial b_j^1} = \\delta_j^2 $\n",
        "\n",
        "### ðŸ”¹ Update Rules:\n",
        "\n",
        "- $ w_{kj}^{2(new)} = w_{kj}^{2(old)} - \\alpha (h_k - y_k) g'(z_k^3) a_j^2 $\n",
        "\n",
        "- $ b_k^{2(new)} = b_k^{2(old)} - \\alpha (h_k - y_k) g'(z_k^3) $\n",
        "\n",
        "- $ w_{ji}^{1(new)} = w_{ji}^{1(old)} - \\alpha \\delta_j^2 x_i $\n",
        "\n",
        "- $ b_j^{1(new)} = b_j^{1(old)} - \\alpha \\delta_j^2 $\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… 2. Mini-Batch Gradient Descent\n",
        "\n",
        "### ðŸ”¹ Loss Function:\n",
        "\n",
        "For a mini-batch of size $ B $:\n",
        "\n",
        "$$\n",
        "J^{(\\text{batch})}(\\mathbf{w}, \\mathbf{b}) = \\frac{1}{2B} \\sum_{q=1}^{B} (h_k^q - y_k^q)^2\n",
        "$$\n",
        "\n",
        "### ðŸ”¹ Gradients:\n",
        "\n",
        "**Average over the batch**:\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial w_{kj}^2} = \\frac{1}{B} \\sum_{q=1}^B (h_k^q - y_k^q) g'(z_k^{3,q}) a_j^{2,q} $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial b_k^2} = \\frac{1}{B} \\sum_{q=1}^B (h_k^q - y_k^q) g'(z_k^{3,q}) $\n",
        "\n",
        "- Hidden error term for each $ j $:  \n",
        "  $$\n",
        "  \\delta_j^{2,q} = f'(z_j^{2,q}) \\sum_k (h_k^q - y_k^q) g'(z_k^{3,q}) w_{kj}^2\n",
        "  $$\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial w_{ji}^1} = \\frac{1}{B} \\sum_{q=1}^B \\delta_j^{2,q} x_i^q $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial b_j^1} = \\frac{1}{B} \\sum_{q=1}^B \\delta_j^{2,q} $\n",
        "\n",
        "### ðŸ”¹ Update Rules:\n",
        "\n",
        "- $ w_{kj}^{2(new)} = w_{kj}^{2(old)} - \\alpha \\cdot \\frac{1}{B} \\sum_{q=1}^B (h_k^q - y_k^q) g'(z_k^{3,q}) a_j^{2,q} $\n",
        "\n",
        "- $ w_{ji}^{1(new)} = w_{ji}^{1(old)} - \\alpha \\cdot \\frac{1}{B} \\sum_{q=1}^B \\delta_j^{2,q} x_i^q $\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RY2q2xVtfF64"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4WM6tIMlkmf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}