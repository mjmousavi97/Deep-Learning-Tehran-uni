{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZom8WYcz3bOLXElorJjJu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjmousavi97/Deep-Learning-Tehran-uni/blob/main/HomeWorks/02%20HW/advanced_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "7--kvYcpUhfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # Core PyTorch library for tensors and GPU computations\n",
        "from torch import nn  # Module for building neural network layers\n",
        "import torch.nn.functional as F  # Functional API for layers and activations (stateless functions)\n",
        "from torch.utils.data import DataLoader, Dataset  # Tools for batching, shuffling, and creating custom datasets\n",
        "\n",
        "import torchvision  # PyTorch package for computer vision datasets, models, and transforms\n",
        "from torchvision import transforms  # Image preprocessing and transformations (resize, normalize, etc.)\n",
        "from torchvision.datasets import ImageFolder  # Loads images from folders organized by class labels\n",
        "\n",
        "import matplotlib.pyplot as plt  # Plotting graphs and displaying images\n",
        "import numpy as np  # Numerical computations and array operations\n",
        "import seaborn as sns  # Statistical data visualization\n",
        "import os  # File and directory operations\n",
        "import glob  # File path pattern matching (wildcards)\n",
        "import cv2  # OpenCV for image processing (reading, editing, filtering, etc.)\n",
        "from tqdm import tqdm  # Progress bar for loops\n",
        "from PIL import Image  # Pillow library for opening, saving, and manipulating images\n"
      ],
      "metadata": {
        "id": "JFjDsAxXUzV3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting Google Drive"
      ],
      "metadata": {
        "id": "3CNgmvyffNxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtdzjcG9fPxn",
        "outputId": "a4f78486-7481-4d67-e67f-6439c8bd645a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "xzTs9JVZS04N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_TRAIN = \"/content/drive/MyDrive/fashionmnist/train/\"\n",
        "DIR_TEST = \"/content/drive/MyDrive/fashionmnist/test/\""
      ],
      "metadata": {
        "id": "YDvL2o2GTnNe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = os.listdir(DIR_TRAIN)\n",
        "print(f\"Names of classes are: {classes}.\\nThere are {len(classes)} classes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LYgWvELUdw1",
        "outputId": "192711c4-4837-4077-82bc-10837cbb1f5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Names of classes are: ['Dress', 'Pullover', 'Sandal', 'Shirt', 'Sneaker', 'T-shirt', 'Trouser', 'Angle boot', 'Bag', 'Coat'].\n",
            "There are 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = []\n",
        "test_images = []\n",
        "\n",
        "for _class in classes:\n",
        "    train_images += glob.glob(DIR_TRAIN + _class + '/*.jpg')\n",
        "    test_images += glob.glob(DIR_TEST + _class + '/*.jpg')\n",
        "\n",
        "print(\"Total train images:\", len(train_images))\n",
        "print(\"Total test images:\", len(test_images))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6w59SoAkEop",
        "outputId": "41d8dd7f-e8db-420f-b4bd-e8f4e1e1496c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train images: 60000\n",
            "Total test images: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                 transforms.Normalize(mean=0.5, std=0.5)])"
      ],
      "metadata": {
        "id": "IRD6n9GxoXXF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root = \"data\", # where to download data to\n",
        "    train = True, # get training data\n",
        "    download = True, # download data if it doesn't exist on disk\n",
        "    transform = Transforms # images come as PIL format, we want to apply transform on them\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = False, # get test data\n",
        "    download = True,\n",
        "    transform = Transforms\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPMwap19vk8N",
        "outputId": "a64c0028-2bab-4c61-8869-f88a58b08623"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 16.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 272kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.09MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 7.54MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataloader"
      ],
      "metadata": {
        "id": "TyaXLrw2vyAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=2,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "eZNDi_wuv24_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_batch, train_label_batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "xc-rMLvZ4HH8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sample_batch.shape, '\\n')\n",
        "print(train_label_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0KGXt314Zkr",
        "outputId": "2f486786-abf6-4312-94a8-eab71edaadb3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28]) \n",
            "\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = train_sample_batch[0].permute(1, 2, 0)\n",
        "label_of_sample_image = train_label_batch[0]\n",
        "\n",
        "mean, std = 0.5, 0.5\n",
        "sample_image = std * (sample_image + mean)\n",
        "\n",
        "plt.imshow(sample_image, cmap='grey')\n",
        "plt.axis('off')\n",
        "plt.title(f'A sample image of train data with label={classes[label_of_sample_image]}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "NN7_F5mW4d6w",
        "outputId": "7522add2-66b6-4a6c-fdd7-e31064482840"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'A sample image of train data with label=T-shirt')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Structure"
      ],
      "metadata": {
        "id": "vutHfpK157yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "[CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n",
        "\n"
      ],
      "metadata": {
        "id": "L4-CTTp1vxkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple Convolutional Neural Network (CNN) model for image classification.\n",
        "\n",
        "    Architecture:\n",
        "    - Two convolutional layers each followed by Batch Normalization and ReLU activation.\n",
        "    - A MaxPooling layer to downsample the feature maps.\n",
        "    - A classifier consisting of Flatten, Dropout, and a Linear (fully connected) layer\n",
        "      to produce the final output.\n",
        "\n",
        "    Args:\n",
        "        input_channel (int): Number of input channels (e.g., 3 for RGB images).\n",
        "        hidden_unit (int): Number of filters (feature maps) in the convolutional layers.\n",
        "        output_shape (int): Number of classes or output features.\n",
        "\n",
        "    Forward pass:\n",
        "        - Input tensor of shape (batch_size, input_channel, height, width).\n",
        "        - Outputs raw logits of shape (batch_size, output_shape).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channel, hidden_unit, output_shape):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature extractor: convolutional block\n",
        "        self.CNN_block = nn.Sequential(\n",
        "            # First convolutional layer\n",
        "            nn.Conv2d(in_channels=input_channel, out_channels=hidden_unit, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=hidden_unit),  # Normalize activations for stability and faster convergence\n",
        "            nn.ReLU(inplace=True),                      # Non-linear activation\n",
        "\n",
        "            # Second convolutional layer\n",
        "            nn.Conv2d(in_channels=hidden_unit, out_channels=hidden_unit, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=hidden_unit),  # Batch normalization again\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # MaxPooling to reduce spatial dimensions by factor of 2\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Classifier: fully connected layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),       # Flatten feature maps into a vector\n",
        "            nn.Dropout(p=0.5),  # Dropout for regularization to prevent overfitting\n",
        "            nn.Linear(in_features=hidden_unit*14*14, out_features=output_shape)  # Final linear layer for classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Defines the forward computation of the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_channel, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape (batch_size, output_shape).\n",
        "        \"\"\"\n",
        "        x = self.CNN_block(x)    # Pass input through convolutional feature extractor\n",
        "        x = self.classifier(x)   # Pass extracted features through classifier\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "HyU2AZaxv1xV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_Model(input_channel=1, hidden_unit=10, output_shape=len(classes))\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fx9UI12GAhN",
        "outputId": "5e8a496f-c5b7-4529-9caa-1df82d89d05b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_Model(\n",
              "  (CNN_block): Sequential(\n",
              "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Dropout(p=0.5, inplace=False)\n",
              "    (2): Linear(in_features=1960, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample = torch.randn((1, 1, 28, 28))\n",
        "model(random_sample).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhdkaHZEGM_v",
        "outputId": "0656af13-8248-4ca3-f9ab-ee532d6fdf06"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "wl5Tab4LHA8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding `with` and `tqdm` in Python\n",
        "\n",
        "## `with` statement\n",
        "- Used to wrap the execution of a block with methods defined by a context manager (`__enter__` and `__exit__`).\n",
        "- Ensures proper setup and cleanup (e.g., opening and closing files, managing resources).\n",
        "- Guarantees cleanup even if errors occur inside the block.\n",
        "- Syntax example:\n",
        "  ```python\n",
        "  with open(\"file.txt\", \"r\") as f:\n",
        "      data = f.read()\n",
        "  ```\n",
        "\n",
        "## `tqdm`\n",
        "- A Python library to display progress bars for iterables.\n",
        "- Wraps any iterable (e.g., lists, ranges, DataLoader) to show progress.\n",
        "- Updates progress bar each iteration, showing completion %, elapsed time, estimated time, etc.\n",
        "- Common usage:\n",
        "  ```python\n",
        "  from tqdm import tqdm\n",
        "\n",
        "  for item in tqdm(iterable, desc=\"Processing\"):\n",
        "      # process item\n",
        "      pass\n",
        "  ```\n",
        "- Can be used with `with` to automatically handle progress bar lifecycle:\n",
        "  ```python\n",
        "  with tqdm(iterable, desc=\"Processing\") as pbar:\n",
        "      for item in pbar:\n",
        "          # process item\n",
        "          pass\n",
        "  ```\n",
        "- When wrapping a DataLoader, tqdm tracks progress over **all batches**, not individual samples.\n"
      ],
      "metadata": {
        "id": "AGmYtzKEy_Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, data_loader, loss_fn, optimizer, accuracy_fn, device):\n",
        "    \"\"\"\n",
        "    Performs one full training epoch over the provided data loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The neural network model to train.\n",
        "        data_loader (DataLoader): DataLoader providing training batches.\n",
        "        loss_fn (callable): Loss function to compute the error.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
        "        accuracy_fn (callable): Function to calculate accuracy.\n",
        "        device (torch.device): Device to run computations on (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_train_loss, average_train_accuracy) over the epoch.\n",
        "    \"\"\"\n",
        "    model.to(device)  # Move model to the specified device (CPU or GPU)\n",
        "    train_loss, train_acc, counter = 0, 0, 0\n",
        "\n",
        "    with tqdm(data_loader, desc='  train') as train_tqdm:\n",
        "        for X, y in train_tqdm:\n",
        "            # Move data to the specified device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass: predict outputs\n",
        "            y_pred = model(X)\n",
        "\n",
        "            # Compute the loss for current batch\n",
        "            loss = loss_fn(y_pred, y)\n",
        "\n",
        "            # Accumulate loss and accuracy values\n",
        "            train_loss += loss.item()  # Convert tensor loss to float before accumulating\n",
        "            train_acc += accuracy_fn(true=y.cpu(), pred=y_pred.cpu())\n",
        "\n",
        "            counter += 1  # Increment batch counter\n",
        "\n",
        "            # Update tqdm progress bar postfix with current average accuracy and loss\n",
        "            train_tqdm.set_postfix(\n",
        "                train_acc=train_acc / counter,\n",
        "                train_loss=train_loss / counter,\n",
        "                refresh=True\n",
        "            )\n",
        "\n",
        "            # Zero gradients before backward pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass: compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Compute average loss and accuracy over all batches\n",
        "        train_loss /= len(data_loader)\n",
        "        train_acc /= len(data_loader)\n",
        "\n",
        "    return train_loss, train_acc\n"
      ],
      "metadata": {
        "id": "LRH9EGguHHlU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ypMuKqcr40Gl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}