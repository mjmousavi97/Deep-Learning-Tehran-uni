{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNu7SyW0EAMYSFN61jCfRSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjmousavi97/Deep-Learning-Tehran-uni/blob/main/HomeWorks/06%20HW/src/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zo-YXmi2iHDR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((256, 256)),\n",
        "\n",
        "    # transform to adding noise\n",
        "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.1),\n",
        "\n",
        "    transforms.RandomRotation((-20, 20)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "c4PNUQxgiNTk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we first use datasets.ImageFolder to create a PyTorch Dataset from images stored in a directory. The directory should be structured such that each class has its own subfolder, for example: data/class0/, data/class1/, etc., where the images of each class are stored. The transform argument applies a series of preprocessing and augmentation operations to each image, such as resizing, normalization, rotation, or adding noise. Each element of the resulting dataset is a tuple (image, label), where image is a tensor of shape [C, H, W] after transformation, and label is an integer representing the class of the image.\n",
        "\n",
        "Next, the dataset is split into training, validation, and test subsets using random_split. This function takes the dataset and a list specifying the number of samples for each subset. It returns three separate Dataset objects, each behaving like the original dataset but containing only the assigned number of samples. It is important that the sum of the subset sizes equals the total number of samples in the dataset to avoid errors.\n",
        "\n",
        "After splitting, we use DataLoader to create iterators for each subset. A DataLoader takes a Dataset and returns batches of data that can be fed directly into a neural network. Important parameters include batch_size, which defines how many samples are in each batch, shuffle, which if set to True randomizes the order of samples at each epoch (recommended for the training set), and num_workers, which specifies how many subprocesses are used to load the data in parallel for faster performance. Each iteration over a DataLoader yields a tuple (inputs, labels), where inputs is a tensor of shape [batch_size, C, H, W] representing a batch of images, and labels is a tensor of shape [batch_size] containing the corresponding class labels."
      ],
      "metadata": {
        "id": "6rfivxIBXPzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root='./data', transform=transform)\n",
        "\n",
        "train_size = int(0.6 * len(dataset))\n",
        "remaining = len(dataset) - train_size\n",
        "val_size = remaining // 2\n",
        "test_size = remaining - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "rAiban5ze1IE",
        "outputId": "c1c18699-453b-4b7f-ce7c-eb11b0fc60cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1879164738.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class bottleneck(nn.Module):\n",
        "    class Bottleneck(nn.Module):\n",
        "    \"\"\"\n",
        "    Bottleneck block used in ResNet-50/101/152.\n",
        "\n",
        "    This block has three convolutional layers:\n",
        "      1. 1x1 conv to reduce channels.\n",
        "      2. 3x3 conv for spatial processing.\n",
        "      3. 1x1 conv to expand channels by the expansion factor (default 4).\n",
        "\n",
        "    A residual (shortcut) connection is added from input to output.\n",
        "    If the input and output shapes don't match, a `downsample` layer can\n",
        "    be provided to adjust them.\n",
        "\n",
        "    :Parameters:\n",
        "\n",
        "    in_channels : int\n",
        "        Number of channels in the input.\n",
        "    out_channels : int\n",
        "        Number of channels in the bottleneck layers. Output will be out_channels * 4.\n",
        "    stride : int, optional\n",
        "        Stride for the 3x3 conv. Defaults to 1.\n",
        "    downsample : nn.Module or None, optional\n",
        "        Layer to match the input to output shape if needed.\n",
        "\n",
        "    :Notes:\n",
        "\n",
        "    Use downsample when stride > 1 or the number of channels changes.\n",
        "    \"\"\"\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stride = stride\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = self.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels*self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "wYPNzaWPPQgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Network (ResNet) implementation.\n",
        "\n",
        "    This class defines the ResNet backbone, composed of an initial convolution\n",
        "    stem, followed by a series of residual blocks (Bottlenecks), global average\n",
        "    pooling, and a fully connected classifier.\n",
        "\n",
        "    :Parameters:\n",
        "\n",
        "    block : nn.Module\n",
        "        Residual block type to be used (e.g., Bottleneck).\n",
        "    layers : list of int\n",
        "        Number of blocks in each of the four main layers.\n",
        "    in_channels : int, optional\n",
        "        Number of input channels to the first convolutional layer (default: 64).\n",
        "    num_classes : int, optional\n",
        "        Number of output classes for the final classification layer (default: 1000).\n",
        "\n",
        "    :Attributes:\n",
        "\n",
        "    conv1 : nn.Conv2d\n",
        "        Initial convolution layer.\n",
        "    bn1 : nn.BatchNorm2d\n",
        "        Batch normalization after the first convolution.\n",
        "    relu : nn.ReLU\n",
        "        ReLU activation function.\n",
        "    maxpool : nn.MaxPool2d\n",
        "        Max pooling layer after the initial stem.\n",
        "    layer1-4 : nn.Sequential\n",
        "        Stacked residual layers with different channel sizes.\n",
        "    avgpool : nn.AdaptiveAvgPool2d\n",
        "        Global average pooling to reduce spatial dimensions to (1,1).\n",
        "    fc : nn.Linear\n",
        "        Fully connected layer mapping features to class scores.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block, layers, in_channels=64, num_classes=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Initial convolution + BN + ReLU + MaxPool (the \"stem\" of the network)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.in_channels,\n",
        "                               kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual layers with different output channel sizes\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # Global average pooling + fully connected classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, self.num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        \"\"\"\n",
        "        Build one stage of the ResNet with stacked residual blocks.\n",
        "\n",
        "        :Parameters:\n",
        "\n",
        "        block : nn.Module\n",
        "            Residual block type to use.\n",
        "        out_channels : int\n",
        "            Number of output channels for the blocks in this layer.\n",
        "        blocks : int\n",
        "            Number of residual blocks to stack.\n",
        "        stride : int, optional\n",
        "            Stride for the first block in the layer (default: 1).\n",
        "\n",
        "        :Returns:\n",
        "\n",
        "        nn.Sequential\n",
        "            A sequential container of residual blocks.\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "\n",
        "        # If dimensions change (due to stride or expansion), add a downsampling layer\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        # First block may need downsampling\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        # Remaining blocks keep the same dimensions\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the ResNet model.\n",
        "\n",
        "        :Parameters:\n",
        "\n",
        "        x : torch.Tensor\n",
        "            Input tensor of shape [batch_size, 3, H, W].\n",
        "\n",
        "        :Returns:\n",
        "\n",
        "        torch.Tensor\n",
        "            Output tensor of shape [batch_size, num_classes].\n",
        "        \"\"\"\n",
        "        # Initial convolutional stem\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Residual layers\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Global pooling + flatten + classifier\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)  # flatten to [batch_size, features]\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "tSlB1KP1gCMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validation(model, train_loader, val_loader, epochs, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train and validate a PyTorch model.\n",
        "\n",
        "    This function performs training and validation for a given model over a specified number of epochs.\n",
        "    During training, it updates the model parameters using the optimizer and computes the loss and accuracy.\n",
        "    During validation, it evaluates the model without updating weights and computes metrics such as\n",
        "    precision, F1 score, and overall accuracy.\n",
        "\n",
        "    :param model: torch.nn.Module\n",
        "        The neural network model to be trained and evaluated.\n",
        "    :param train_loader: DataLoader\n",
        "        PyTorch DataLoader for the training dataset.\n",
        "    :param val_loader: DataLoader\n",
        "        PyTorch DataLoader for the validation dataset.\n",
        "    :param epochs: int\n",
        "        Number of epochs to train the model.\n",
        "    :param optimizer: torch.optim.Optimizer\n",
        "        Optimizer used for updating model parameters.\n",
        "    :param criterion: torch.nn.modules.loss._Loss\n",
        "        Loss function used to compute the training and validation loss.\n",
        "    :param device: torch.device\n",
        "        Device to perform computations on (CPU or CUDA).\n",
        "\n",
        "    :returns: tuple of lists\n",
        "        train_loss_history, val_loss_history, train_acc_history, val_acc_history\n",
        "        Lists containing the loss and accuracy values for each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # set model to training mode\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()       # reset gradients\n",
        "            outputs = model(inputs)     # forward pass\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()             # compute gradients\n",
        "            optimizer.step()            # update weights\n",
        "\n",
        "            running_loss += loss.item()  # accumulate batch loss\n",
        "            _, preds = torch.max(outputs, 1)  # predicted classes\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = running_corrects / total\n",
        "        train_loss_history.append(epoch_loss)\n",
        "        train_acc_history.append(epoch_acc)\n",
        "\n",
        "        model.eval()  # set model to evaluation mode\n",
        "        val_running_loss = 0.0\n",
        "        val_running_corrects = 0\n",
        "        val_total = 0\n",
        "\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():  # disable gradient computation\n",
        "            for val_inputs, val_labels in val_loader:\n",
        "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "\n",
        "                val_outputs = model(val_inputs)\n",
        "                val_loss = criterion(val_outputs, val_labels)\n",
        "\n",
        "                val_running_loss += val_loss.item()\n",
        "                _, val_preds = torch.max(val_outputs, 1)\n",
        "                val_running_corrects += (val_preds == val_labels).sum().item()\n",
        "                val_total += val_labels.size(0)\n",
        "\n",
        "                all_preds.extend(val_preds.cpu().numpy())\n",
        "                all_labels.extend(val_labels.cpu().numpy())\n",
        "\n",
        "        val_epoch_loss = val_running_loss / len(val_loader)\n",
        "        val_epoch_acc = val_running_corrects / val_total\n",
        "\n",
        "        val_loss_history.append(val_epoch_loss)\n",
        "        val_acc_history.append(val_epoch_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "              f\"Train Loss: {epoch_loss:.4f} - Train Acc: {epoch_acc:.4f} - \"\n",
        "              f\"Val Loss: {val_epoch_loss:.4f} - Val Acc: {val_epoch_acc:.4f}\")\n",
        "\n",
        "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        print(f\"Precision: {precision:.4f} - F1 Score: {f1:.4f} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history\n"
      ],
      "metadata": {
        "id": "mn-XdgJd9rVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Resnet_model = ResNet(block=bottleneck, layers=[3, 4, 6, 3], num_classes=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "Resnet_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(Resnet_model.parameters(), lr=0.001)\n",
        "\n",
        "train_loss_history, val_loss_history, train_acc_history, val_acc_history = train_and_validation(Resnet_model, train_loader, val_loader, epochs=2, optimizer=optimizer, criterion=criterion, device=device)"
      ],
      "metadata": {
        "id": "Pp_0572QYNKQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}