{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjjx7soYb2voNROXyfEulB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjmousavi97/Deep-Learning-Tehran-uni/blob/main/HomeWorks/03%20HW/src/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HUD7kKamDDQO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from timeit import default_timer as timer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transformation pipeline for preprocessing images\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        # 1. Convert the input image (PIL Image or NumPy array) into a PyTorch tensor.\n",
        "        #    - Converts pixel values from range [0, 255] to [0.0, 1.0].\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "        # 2. Randomly flip the image horizontally with probability 0.5.\n",
        "        #    - Useful for data augmentation to improve model generalization.\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "        # 3. Normalize the image tensor using mean=0.5 and std=0.5.\n",
        "        #    - Formula: (x - mean) / std\n",
        "        #    - This maps values from [0.0, 1.0] to [-1.0, 1.0].\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ]\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "Docstring:\n",
        "----------\n",
        "This transformation pipeline is designed for preprocessing image data\n",
        "before feeding it into a neural network.\n",
        "\n",
        "Steps applied to each image:\n",
        "1. Convert the image to a tensor with values in [0,1].\n",
        "2. Apply a random horizontal flip with probability 0.5\n",
        "   (data augmentation).\n",
        "3. Normalize the tensor so that pixel values fall in [-1,1].\n",
        "\n",
        "Typical use case: applied to image datasets (e.g., MNIST, CIFAR-10)\n",
        "to improve training stability and generalization.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "VsCj88zlZVej",
        "outputId": "1046dc47-1ca6-4d25-9f70-fddceffd0862"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDocstring:\\n----------\\nThis transformation pipeline is designed for preprocessing image data \\nbefore feeding it into a neural network.\\n\\nSteps applied to each image:\\n1. Convert the image to a tensor with values in [0,1].\\n2. Apply a random horizontal flip with probability 0.5 \\n   (data augmentation).\\n3. Normalize the tensor so that pixel values fall in [-1,1].\\n\\nTypical use case: applied to image datasets (e.g., MNIST, CIFAR-10)\\nto improve training stability and generalization.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FashionMNIST training dataset\n",
        "train_dataset_FashionMNIST = torchvision.datasets.FashionMNIST(\n",
        "    root=\"data\",          # Directory where the dataset will be stored/downloaded\n",
        "    train=True,           # True = load the training split (60,000 images)\n",
        "    download=True,        # Download the dataset if it's not already on disk\n",
        "    transform=transform   # Apply preprocessing pipeline (ToTensor, augmentation, normalization)\n",
        ")\n",
        "\n",
        "# Load the FashionMNIST test dataset\n",
        "test_dataset_FashionMNIST = torchvision.datasets.FashionMNIST(\n",
        "    root=\"data\",          # Same directory used for training dataset\n",
        "    train=False,          # False = load the test split (10,000 images)\n",
        "    download=True,        # Download if not already present\n",
        "    transform=transform   # Apply the same preprocessing pipeline as training\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCQWTQuRZVbJ",
        "outputId": "a8622430-6e9a-49bf-cd63-8faa194a383a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.6MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 202kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.75MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 9.59MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST training dataset\n",
        "train_dataset_MNIST = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Load the MNIST test dataset\n",
        "test_dataset_MNIST = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGpNiJUNZVYc",
        "outputId": "880fcd31-78a5-4be8-8f9a-446a0d97be22"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 456kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.23MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.79MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR10 training dataset\n",
        "train_dataset_CIFAR10 = torchvision.datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Load the CIFAR10 test dataset\n",
        "test_dataset_CIFAR10 = torchvision.datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIr3Pj9OZVVf",
        "outputId": "29232e07-c18a-4371-f6b5-df2bd20357a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(train_raw_dataset, test_raw_dataset, batch_size):\n",
        "    \"\"\"\n",
        "    Create PyTorch DataLoaders for training and testing datasets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_raw_dataset : torch.utils.data.Dataset\n",
        "        The training dataset (e.g., MNIST, CIFAR-10).\n",
        "    test_raw_dataset : torch.utils.data.Dataset\n",
        "        The test dataset (e.g., MNIST, CIFAR-10).\n",
        "    batch_size : int\n",
        "        Number of samples per batch to load.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_dataloader : torch.utils.data.DataLoader\n",
        "        DataLoader for the training dataset with shuffling enabled.\n",
        "    test_dataloader : torch.utils.data.DataLoader\n",
        "        DataLoader for the test dataset with shuffling disabled.\n",
        "    \"\"\"\n",
        "    train_dataloader = DataLoader(\n",
        "        dataset=train_raw_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2,   # number of subprocesses to use for data loading\n",
        "        shuffle=True     # shuffle only for training\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        dataset=test_raw_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2,\n",
        "        shuffle=False    # don't shuffle test data\n",
        "    )\n",
        "\n",
        "    return train_dataloader, test_dataloader\n"
      ],
      "metadata": {
        "id": "ugXOqmq1ZVSq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders for FashionMNIST\n",
        "train_dataloader_FashionMNIST, test_dataloader_FashionMNIST = create_dataloader(\n",
        "    train_dataset_FashionMNIST, test_dataset_FashionMNIST, batch_size=128\n",
        ")\n",
        "\n",
        "# Create dataloaders for MNIST\n",
        "train_dataloader_MNIST, test_dataloader_MNIST = create_dataloader(\n",
        "    train_dataset_MNIST, test_dataset_MNIST, batch_size=128\n",
        ")\n",
        "\n",
        "# Create dataloaders for CIFAR-10\n",
        "train_dataloader_CIFAR10, test_dataloader_CIFAR10 = create_dataloader(\n",
        "    train_dataset_CIFAR10, test_dataset_CIFAR10, batch_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "RD7mpbJxZVPp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FashionMNIST classes:\", train_dataset_FashionMNIST.classes)\n",
        "print(\"MNIST classes:\", train_dataset_MNIST.classes)\n",
        "print(\"CIFAR-10 classes:\", train_dataset_CIFAR10.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrus9kyjZVM4",
        "outputId": "9e733ad5-1d0f-4a49-97bf-d397bff1a677"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FashionMNIST classes: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
            "MNIST classes: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
            "CIFAR-10 classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample(data_dataloader, mean, std):\n",
        "    batch_images, batch_labels = next(iter(data_dataloader))\n",
        "    normed_image = batch_images[0]      # first image in batch\n",
        "    label = batch_labels[0]             # its label\n",
        "\n",
        "    # Denormalize: x * std + mean\n",
        "    original_image = (normed_image * std) + mean\n",
        "\n",
        "    # Remove channel dimension (1, H, W) -> (H, W)\n",
        "    plt.imshow(original_image.permute(1, 2, 0), cmap=\"gray\")\n",
        "    plt.title(f\"Label: {label.item()}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    print(f\"Image size: {original_image.shape}\")"
      ],
      "metadata": {
        "id": "eCwXWKEOZVJ4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample(train_dataloader_FashionMNIST, mean=0.5, std=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "I86WUqxdZU6V",
        "outputId": "e1b286c4-776c-40a1-a497-74557e0cec2d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample(train_dataloader_MNIST, mean=0.5, std=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "qqzHCByqZUvZ",
        "outputId": "52235eb1-6546-4f01-c323-d0e4f7714c06"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample(train_dataloader_CIFAR10, mean=0.5, std=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "5UvybEU6yYm8",
        "outputId": "1f6ce934-9910-42e2-d057-51e73c51e808"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: torch.Size([3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Model"
      ],
      "metadata": {
        "id": "xB1YLKXM_O6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SCNNB(nn.Module):\n",
        "    \"\"\"\n",
        "    Shallow Convolutional Neural Network with Batch Normalization (SCNNB).\n",
        "\n",
        "    Architecture (for input 28x28x1 like MNIST):\n",
        "        - Conv(3x3, 32 filters, padding=1) + BN + ReLU\n",
        "        - MaxPool(2x2)\n",
        "        - Conv(3x3, 64 filters, padding=1) + BN + ReLU\n",
        "        - MaxPool(2x2)\n",
        "        - Flatten\n",
        "        - FC(3136 -> 1280) + ReLU\n",
        "        - Dropout(0.5)\n",
        "        - FC(1280 -> num_classes)\n",
        "\n",
        "    Notes:\n",
        "        - Padding=1 keeps spatial size before pooling.\n",
        "        - For MNIST (1x28x28) → Flatten=3136 (64*7*7).\n",
        "        - For CIFAR-10 (3x32x32) → Flatten=4096 (64*8*8).\n",
        "        - Forward returns raw logits; use nn.CrossEntropyLoss for training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int = 1, num_classes: int = 10, input_size: int = 28):\n",
        "        \"\"\"\n",
        "        Initialize the SCNNB model.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels (1 for grayscale, 3 for RGB).\n",
        "            num_classes (int): Number of output classes.\n",
        "            input_size (int): Height/Width of the input image (assumed square).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_size  = input_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # ---- Feature extractor ----\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # ---- Classifier ----\n",
        "        feat_dim   = self._calc_feat_dim()  # Dynamically compute flattened feature dimension\n",
        "        self.fc1   = nn.Linear(feat_dim, 1280)\n",
        "        self.drop1 = nn.Dropout(p=0.5)\n",
        "        self.fc_out= nn.Linear(1280, num_classes)\n",
        "\n",
        "    def _calc_feat_dim(self) -> int:\n",
        "        \"\"\"\n",
        "        Compute the number of features after the convolution + pooling layers.\n",
        "        This ensures the fully connected layer is correctly sized for any input.\n",
        "\n",
        "        Returns:\n",
        "            int: Flattened feature dimension.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            device = next(self.parameters()).device  # Match the device of the model\n",
        "            tmp = torch.zeros((1, self.in_channels, self.input_size, self.input_size), device=device)\n",
        "            tmp = self.pool1(F.relu(self.bn1(self.conv1(tmp))))\n",
        "            tmp = self.pool2(F.relu(self.bn2(self.conv2(tmp))))\n",
        "            return tmp.numel()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of SCNNB.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (N, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits of shape (N, num_classes).\n",
        "        \"\"\"\n",
        "        # ---- Feature extraction ----\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "\n",
        "        # ---- Classification head ----\n",
        "        x = torch.flatten(x, start_dim=1)   # Flatten to (N, feat_dim)\n",
        "        x = F.relu(self.fc1(x))             # Fully connected hidden layer\n",
        "        x = self.drop1(x)                   # Dropout for regularization\n",
        "        logits = self.fc_out(x)             # Output logits (no softmax)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "N7OIH-cfDsQQ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes_MNIST = len(train_dataset_MNIST.classes)\n",
        "num_classes_FashionMNIST = len(train_dataset_FashionMNIST.classes)\n",
        "num_classes_CIFAR10 = len(train_dataset_CIFAR10.classes)"
      ],
      "metadata": {
        "id": "ckx5r2SL3hPe"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modell = SCNNB(in_channels=1, num_classes=num_classes_MNIST, input_size=28)\n",
        "model2 = SCNNB(in_channels=1, num_classes=num_classes_FashionMNIST, input_size=28)\n",
        "model3 = SCNNB(in_channels=3, num_classes=num_classes_CIFAR10, input_size=32)"
      ],
      "metadata": {
        "id": "qQ685UL2PEif"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample = torch.randn((1, 3, 32, 32))\n",
        "model3(random_sample).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWzwj0c44JPL",
        "outputId": "010fd4ec-a883-499a-a420-2e9c7a2d1570"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Model"
      ],
      "metadata": {
        "id": "QkLoyLdB58Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SCNNB_a(nn.Module):\n",
        "    \"\"\"\n",
        "    Shallow Convolutional Neural Network with automatic feature dimension calculation.\n",
        "\n",
        "    Architecture:\n",
        "        - Two convolutional layers with ReLU activation and MaxPooling\n",
        "        - Batch normalization after the second convolution\n",
        "        - Fully connected classifier with one hidden layer, ReLU, and Dropout\n",
        "        - Output layer produces logits for classification\n",
        "\n",
        "    Attributes:\n",
        "        in_channels (int): Number of channels in the input image (e.g., 1 for grayscale, 3 for RGB)\n",
        "        image_size (int): Height and width of the square input image\n",
        "        num_classes (int): Number of output classes for classification\n",
        "        CNN_block (nn.Sequential): Feature extraction layers (convolutions + activations + pooling)\n",
        "        classifier (nn.Sequential): Fully connected layers for classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=1, num_classes=10, image_size=28):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.image_size = image_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Feature extraction block\n",
        "        self.CNN_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Fully connected classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=self._calc_feat_dim(), out_features=1280),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=1280, out_features=self.num_classes)\n",
        "        )\n",
        "\n",
        "    def _calc_feat_dim(self) -> int:\n",
        "        \"\"\"\n",
        "        Compute the flattened feature dimension after CNN_block.\n",
        "\n",
        "        This ensures the linear layer is correctly sized for any input image size.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of features after flattening the CNN output\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            device = next(self.parameters()).device  # match the model's device\n",
        "            tmp = torch.zeros((1, self.in_channels, self.image_size, self.image_size), device=device)\n",
        "            tmp = self.CNN_block(tmp)\n",
        "            return tmp.numel()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, image_size, image_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.CNN_block(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "v8DnBPhw_Vc1"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third Model"
      ],
      "metadata": {
        "id": "i2rqmqG1IShW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SCNNB_b(nn.Module):\n",
        "    \"\"\"\n",
        "    Shallow Convolutional Neural Network with automatic feature dimension calculation.\n",
        "\n",
        "    Architecture:\n",
        "        - Two convolutional layers with ReLU activation and MaxPooling\n",
        "        - No Batch Normalization\n",
        "        - Fully connected classifier with one hidden layer, ReLU, and Dropout\n",
        "        - Output layer produces logits for classification\n",
        "\n",
        "    Attributes:\n",
        "        in_channels (int): Number of channels in the input image (e.g., 1 for grayscale, 3 for RGB)\n",
        "        image_size (int): Height and width of the square input image\n",
        "        num_classes (int): Number of output classes for classification\n",
        "        CNN_block (nn.Sequential): Feature extraction layers (convolutions + activations + pooling)\n",
        "        classifier (nn.Sequential): Fully connected layers for classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=1, num_classes=10, image_size=28):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.image_size = image_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Feature extraction block\n",
        "        self.CNN_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Fully connected classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=self._calc_feat_dim(), out_features=1280),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=1280, out_features=self.num_classes)\n",
        "        )\n",
        "\n",
        "    def _calc_feat_dim(self) -> int:\n",
        "        \"\"\"\n",
        "        Compute the flattened feature dimension after CNN_block.\n",
        "\n",
        "        This ensures the linear layer is correctly sized for any input image size.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of features after flattening the CNN output\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            device = next(self.parameters()).device  # match the model's device\n",
        "            tmp = torch.zeros((1, self.in_channels, self.image_size, self.image_size), device=device)\n",
        "            tmp = self.CNN_block(tmp)\n",
        "            return tmp.numel()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, image_size, image_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.CNN_block(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5r5q_4ToIy5r"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vE6hCiRcJEg6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}