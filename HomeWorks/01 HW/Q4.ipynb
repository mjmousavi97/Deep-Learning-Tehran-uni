{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrRl0azNGToKlsnO1OkL8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjmousavi97/Deep-Learning-Tehran-uni/blob/main/HomeWorks/01%20HW/Q4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Process of MLP with One Hidden Layer (Backpropagation)\n",
        "\n",
        "This document provides a detailed derivation and explanation of the backpropagation learning algorithm for a Multi-Layer Perceptron (MLP) with **one hidden layer**, trained using the **Sum of Squared Errors (SSE)** loss.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”· Network Architecture\n",
        "\n",
        "We consider a standard feedforward neural network with:\n",
        "- One input layer\n",
        "- One hidden layer with activation function $f$\n",
        "- One output layer with activation function $g$ (can be identity for regression)\n",
        "\n",
        "The forward propagation formula is:\n",
        "\n",
        "$$\n",
        "h_{\\mathbf{w}, \\mathbf{b}}(\\mathbf{x}) = g\\left(W^2 f\\left(W^1 \\mathbf{x} + \\mathbf{b}^1 \\right) + \\mathbf{b}^2\\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $W^1$: weights from input to hidden layer  \n",
        "- $b^1$: biases of hidden layer  \n",
        "- $W^2$: weights from hidden to output layer  \n",
        "- $b^2$: biases of output layer  \n",
        "- $f$: activation function of hidden layer  \n",
        "- $g$: activation function of output layer (e.g., identity for regression)\n",
        "\n",
        "\n",
        "\n",
        "## ðŸ”· Training Set\n",
        "\n",
        "$$\n",
        "\\left\\{ (\\mathbf{x}^q, y^q) \\right\\}_{q=1}^Q\n",
        "$$\n",
        "\n",
        "- $\\mathbf{x}^q \\in \\mathbb{R}^n$: input vector  \n",
        "- $y^q \\in \\mathbb{R}$: target scalar output\n",
        "\n",
        "\n",
        "\n",
        "## ðŸ”· Loss Function (Sum of Squared Errors)\n",
        "\n",
        "$$\n",
        "J(\\mathbf{w}, \\mathbf{b}) = \\frac{1}{2} \\sum_{q=1}^{Q} \\left\\| h_{\\mathbf{w},\\mathbf{b}}(\\mathbf{x}^q) - y^q \\right\\|^2\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## ðŸ”· Backpropagation Derivations\n",
        "\n",
        "We derive gradients for each parameter using the chain rule. Let:\n",
        "- $z_j^2 = \\sum_i w_{ji}^1 x_i + b_j^1$\n",
        "- $a_j^2 = f(z_j^2)$\n",
        "- $z_k^3 = \\sum_j w_{kj}^2 a_j^2 + b_k^2$\n",
        "- $h_k = g(z_k^3)$\n",
        "\n",
        "\n",
        "\n",
        "### âœ… Gradient of Loss w.r.t. Output Weights $w_{kj}^2$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_{kj}^2} = \\frac{\\partial J}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial z_k^3} \\cdot \\frac{\\partial z_k^3}{\\partial w_{kj}^2}\n",
        "= (h_k - y_k) g'(z_k^3) a_j^2\n",
        "$$\n",
        "\n",
        "Update rule:\n",
        "$$\n",
        "w_{kj}^{2(new)} = w_{kj}^{2(old)} - \\alpha (h_k - y_k) g'(z_k^3) a_j^2\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### âœ… Gradient of Loss w.r.t. Output Bias $b_k^2$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_k^2} = \\frac{\\partial J}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial z_k^3} \\cdot \\frac{\\partial z_k^3}{\\partial b_k^2} = (h_k - y_k) g'(z_k^3)\n",
        "$$\n",
        "\n",
        "Update rule:\n",
        "$$\n",
        "b_k^{2(new)} = b_k^{2(old)} - \\alpha (h_k - y_k) g'(z_k^3)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### âœ… Gradient of Loss w.r.t. Hidden Weights $w_{ji}^1$\n",
        "\n",
        "We apply the chain rule through the network:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_{ji}^1} = \\sum_{k=1}^r \\left( \\frac{\\partial J}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial z_k^3} \\cdot \\frac{\\partial z_k^3}{\\partial a_j^2} \\cdot \\frac{\\partial a_j^2}{\\partial z_j^2} \\cdot \\frac{\\partial z_j^2}{\\partial w_{ji}^1} \\right)\n",
        "$$\n",
        "\n",
        "Plugging in each derivative:\n",
        "- $\\frac{\\partial J}{\\partial h_k} = h_k - y_k$\n",
        "- $\\frac{\\partial h_k}{\\partial z_k^3} = g'(z_k^3)$\n",
        "- $\\frac{\\partial z_k^3}{\\partial a_j^2} = w_{kj}^2$\n",
        "- $\\frac{\\partial a_j^2}{\\partial z_j^2} = f'(z_j^2)$\n",
        "- $\\frac{\\partial z_j^2}{\\partial w_{ji}^1} = x_i$\n",
        "\n",
        "Final expression:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_{ji}^1} = \\left( \\sum_{k=1}^{r} (h_k - y_k) g'(z_k^3) w_{kj}^2 \\right) f'(z_j^2) x_i\n",
        "$$\n",
        "\n",
        "Update rule:\n",
        "$$\n",
        "w_{ji}^{1(new)} = w_{ji}^{1(old)} - \\alpha \\left( \\sum_{k=1}^{r} (h_k - y_k) g'(z_k^3) w_{kj}^2 \\right) f'(z_j^2) x_i\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### âœ… Gradient of Loss w.r.t. Hidden Bias $b_j^1$\n",
        "\n",
        "Same logic, except no $x_i$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_j^1} = \\left( \\sum_{k=1}^{r} (h_k - y_k) g'(z_k^3) w_{kj}^2 \\right) f'(z_j^2)\n",
        "$$\n",
        "\n",
        "Update rule:\n",
        "$$\n",
        "b_j^{1(new)} = b_j^{1(old)} - \\alpha \\left( \\sum_{k=1}^{r} (h_k - y_k) g'(z_k^3) w_{kj}^2 \\right) f'(z_j^2)\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eZRe6gwZe-PT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation Derivations for Stochastic and Mini-Batch Gradient Descent\n",
        "\n",
        "This document provides detailed mathematical derivations for training a Multi-Layer Perceptron (MLP) with one hidden layer using:\n",
        "\n",
        "- **Stochastic Gradient Descent (SGD)**\n",
        "- **Mini-Batch Gradient Descent**\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Network Architecture\n",
        "\n",
        "Let the MLP be defined as follows:\n",
        "\n",
        "- Input: $ \\mathbf{x} \\in \\mathbb{R}^n $\n",
        "- Hidden layer: weights $ W^1 $, biases $ b^1 $, activation $ f $\n",
        "- Output layer: weights $ W^2 $, biases $ b^2 $, activation $ g $\n",
        "\n",
        "### Forward Propagation:\n",
        "\n",
        "- Hidden pre-activation:  \n",
        "  $$\n",
        "  z_j^2 = \\sum_{i} w_{ji}^1 x_i + b_j^1\n",
        "  $$\n",
        "\n",
        "- Hidden activation:  \n",
        "  $$\n",
        "  a_j^2 = f(z_j^2)\n",
        "  $$\n",
        "\n",
        "- Output pre-activation:  \n",
        "  $$\n",
        "  z_k^3 = \\sum_{j} w_{kj}^2 a_j^2 + b_k^2\n",
        "  $$\n",
        "\n",
        "- Output activation:  \n",
        "  $$\n",
        "  h_k = g(z_k^3)\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… 1. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "### ðŸ”¹ Loss Function:\n",
        "\n",
        "For a single training example $ (\\mathbf{x}^q, y^q) $:\n",
        "\n",
        "$$\n",
        "J^{(q)}(\\mathbf{w}, \\mathbf{b}) = \\frac{1}{2} (h_k^q - y_k^q)^2\n",
        "$$\n",
        "\n",
        "### ðŸ”¹ Gradients:\n",
        "\n",
        "#### Output Layer\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial z_k^3} = (h_k - y_k) \\cdot g'(z_k^3) $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial w_{kj}^2} = (h_k - y_k) g'(z_k^3) a_j^2 $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial b_k^2} = (h_k - y_k) g'(z_k^3) $\n",
        "\n",
        "#### Hidden Layer\n",
        "\n",
        "- Backpropagate error:  \n",
        "  $$\n",
        "  \\delta_j^2 = f'(z_j^2) \\sum_k (h_k - y_k) g'(z_k^3) w_{kj}^2\n",
        "  $$\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial w_{ji}^1} = \\delta_j^2 x_i $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial b_j^1} = \\delta_j^2 $\n",
        "\n",
        "### ðŸ”¹ Update Rules:\n",
        "\n",
        "- $ w_{kj}^{2(new)} = w_{kj}^{2(old)} - \\alpha (h_k - y_k) g'(z_k^3) a_j^2 $\n",
        "\n",
        "- $ b_k^{2(new)} = b_k^{2(old)} - \\alpha (h_k - y_k) g'(z_k^3) $\n",
        "\n",
        "- $ w_{ji}^{1(new)} = w_{ji}^{1(old)} - \\alpha \\delta_j^2 x_i $\n",
        "\n",
        "- $ b_j^{1(new)} = b_j^{1(old)} - \\alpha \\delta_j^2 $\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… 2. Mini-Batch Gradient Descent\n",
        "\n",
        "### ðŸ”¹ Loss Function:\n",
        "\n",
        "For a mini-batch of size $ B $:\n",
        "\n",
        "$$\n",
        "J^{(\\text{batch})}(\\mathbf{w}, \\mathbf{b}) = \\frac{1}{2B} \\sum_{q=1}^{B} (h_k^q - y_k^q)^2\n",
        "$$\n",
        "\n",
        "### ðŸ”¹ Gradients:\n",
        "\n",
        "**Average over the batch**:\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial w_{kj}^2} = \\frac{1}{B} \\sum_{q=1}^B (h_k^q - y_k^q) g'(z_k^{3,q}) a_j^{2,q} $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial b_k^2} = \\frac{1}{B} \\sum_{q=1}^B (h_k^q - y_k^q) g'(z_k^{3,q}) $\n",
        "\n",
        "- Hidden error term for each $ j $:  \n",
        "  $$\n",
        "  \\delta_j^{2,q} = f'(z_j^{2,q}) \\sum_k (h_k^q - y_k^q) g'(z_k^{3,q}) w_{kj}^2\n",
        "  $$\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial w_{ji}^1} = \\frac{1}{B} \\sum_{q=1}^B \\delta_j^{2,q} x_i^q $\n",
        "\n",
        "- $ \\frac{\\partial J}{\\partial b_j^1} = \\frac{1}{B} \\sum_{q=1}^B \\delta_j^{2,q} $\n",
        "\n",
        "### ðŸ”¹ Update Rules:\n",
        "\n",
        "- $ w_{kj}^{2(new)} = w_{kj}^{2(old)} - \\alpha \\cdot \\frac{1}{B} \\sum_{q=1}^B (h_k^q - y_k^q) g'(z_k^{3,q}) a_j^{2,q} $\n",
        "\n",
        "- $ w_{ji}^{1(new)} = w_{ji}^{1(old)} - \\alpha \\cdot \\frac{1}{B} \\sum_{q=1}^B \\delta_j^{2,q} x_i^q $\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RY2q2xVtfF64"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4WM6tIMlkmf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}